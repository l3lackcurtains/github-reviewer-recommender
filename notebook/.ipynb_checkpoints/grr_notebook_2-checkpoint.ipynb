{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Assign the constants with user defined values.\n",
    "- **ACCESS_TOKEN**: Github token to use github API\n",
    "- **REPO**: Name of github repository\n",
    "- **limit_pr**: Limit the number of closed PR to use\n",
    "- **open_pr_id**: Open PR ID to get recommendation\n",
    "- **similarity_threshold**: Threshold to use top similar PRs\n",
    "\n",
    "It will fetch the closed PRs required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "ACCESS_TOKEN = '5feeb874084cea318f3c5cbf56b70bce1a578290'\n",
    "REPO = 'sveltejs/svelte'\n",
    "limit_pr = None\n",
    "open_pr_id = 1410\n",
    "\n",
    "# Get the access to Github API\n",
    "client = Github(ACCESS_TOKEN, per_page=300)\n",
    "\n",
    "# Get the repository object from Github API\n",
    "repo = client.get_repo(REPO)\n",
    "\n",
    "# Get the maintainer of the repo\n",
    "repo_maintainer = repo.full_name.split(\"/\")[0]\n",
    "\n",
    "# Get all the closed pull requests\n",
    "closed_prs = list(repo.get_pulls(state='closed'))\n",
    "\n",
    "# Limit number of pull requests if limit_pr is set\n",
    "if limit_pr != None:\n",
    "    closed_prs = closed_prs[:limit_pr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Scarp the necessary closed pull requests data from the repository using github api.\n",
    "Store the data in variables for further usage to prevent github api request rate limit.\n",
    "Build a bipartite graph using the data of open PRs and respective reviewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a graph\n",
    "graphz = nx.Graph()\n",
    "\n",
    "# It inserts all the reviewers node we add to graph\n",
    "closed_prs_reviewers = []\n",
    "\n",
    "# Save the data loaded from API for future use\n",
    "closed_prs_meta = []\n",
    "\n",
    "# Iterate through all the closed pull requests\n",
    "for pr in closed_prs:\n",
    "\n",
    "    # If PR doesnt have comments continue with next\n",
    "    if pr.get_issue_comments().totalCount == 0:\n",
    "        continue\n",
    "\n",
    "    # Get the user who submitted this PR\n",
    "    pull_requester = pr.user.login\n",
    "\n",
    "    # Get the PR number\n",
    "    pr_number = 'PR #' + str(pr.number)\n",
    "\n",
    "    ## Insert PR into graph node\n",
    "    graphz.add_node(pr_number, type='Pull Request', bipartite=0)\n",
    "\n",
    "    # Get all the comments of the PR\n",
    "    comments = pr.get_issue_comments()\n",
    "\n",
    "    # Get the meta data from PR and insert in closed_prs_meta\n",
    "    pr_data = {}\n",
    "    pr_data['id'] = pr_number\n",
    "    pr_data['title'] = pr.title\n",
    "    pr_data['body'] = pr.body\n",
    "    pr_data['comments'] = comments\n",
    "    closed_prs_meta.append(pr_data)\n",
    "\n",
    "    # Iterate through all the comments\n",
    "    for comment in comments:\n",
    "\n",
    "        # Exclude user who are bots, maintainer, or PR submitter\n",
    "        if comment.user != None and 'bot' not in comment.user.login and repo_maintainer != comment.user.login and pull_requester != comment.user.login:\n",
    "\n",
    "            # Get the reviewer from comment\n",
    "            reviewer = comment.user.login\n",
    "\n",
    "            # Insert reviewer into graph node and closed_prs_reviewers list\n",
    "            if reviewer not in closed_prs_reviewers:\n",
    "                closed_prs_reviewers.append(reviewer)\n",
    "                graphz.add_node(reviewer, type='user', bipartite=1)\n",
    "\n",
    "            # If there is occurence of multiple comment, then add the occurence to the edge weight\n",
    "            if graphz.has_edge(reviewer, pr_number):\n",
    "                new_weight = graphz.get_edge_data(\n",
    "                    reviewer, pr_number)['weight'] + 1\n",
    "                graphz[reviewer][pr_number]['weight'] = new_weight\n",
    "            else:\n",
    "                graphz.add_edge(reviewer, pr_number,\n",
    "                                weight=1, type='reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "Visualize the bipartite graph where blue nodes are closed PRs and red nodes are reviewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=300)\n",
    "plt.rcParams['legend.loc'] = 'upper right'\n",
    "\n",
    "pos = nx.bipartite_layout(graphz, closed_prs_reviewers)\n",
    "\n",
    "node_colors = [ 'blue' for node in graphz.nodes()]\n",
    "for i, node in enumerate(graphz.nodes):\n",
    "    color = 'blue'\n",
    "    if node in closed_prs_reviewers:\n",
    "        color = \"red\"\n",
    "    node_colors[i] = color\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title(\"Bipartite graph between reviewer and closed pull requests\")\n",
    "plt.plot([0],[0], color=\"blue\", label='Closed Pull Requests')\n",
    "plt.plot([0],[0], color=\"red\", label='Reviewers')\n",
    "plt.legend()\n",
    "\n",
    "edges = graphz.edges()\n",
    "weights = [graphz[u][v]['weight']/10 for u,v in edges]\n",
    "\n",
    "nx.draw_networkx(graphz, pos=pos, node_size=5, node_color=node_colors,\n",
    "                with_labels=False, width=weights, font_size=6, edge_color=\"#666666\")\n",
    "\n",
    "figname = 'bipartite_graph'\n",
    "plt.savefig(figname+'.png', bbox_inches='tight', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Get the information about the bipartite graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(graphz))\n",
    "print(\"Number of reviewers\", len(closed_prs_reviewers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Build a document corpus for all the closed PRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Generate document corpus for closed pull requests\n",
    "closed_prs_corpus = {}\n",
    "for pr in closed_prs_meta:\n",
    "    title = str(pr['title'])\n",
    "    body = str(pr['body'])\n",
    "    doc = title + \" \" + body\n",
    "    for comment in pr['comments']:\n",
    "        doc += comment.body\n",
    "    # Remove the code, mentions and URLS\n",
    "    doc = re.sub(r'\\```[^```]*\\```', '', doc)\n",
    "    doc = re.sub(r\"(?:\\@|#|https?\\://)\\S+\", \"\", doc)\n",
    "\n",
    "    # insert document into corpus with index of corpus id\n",
    "    closed_prs_corpus[pr['id']] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "\n",
    "Get the open pull request and build a document corpus for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of closed PRS\n",
    "open_prs = list(repo.get_pulls(state='open', sort='created'))\n",
    "if len(open_prs) == 0:\n",
    "    raise Exception(\"No open PRs found\")\n",
    "\n",
    "# Get the first open PR\n",
    "open_pr = open_prs[0]\n",
    "\n",
    "# If Id is provided in function, choose this one\n",
    "if open_pr_id != None:\n",
    "    for pr in open_prs:\n",
    "        if open_pr_id == pr.number:\n",
    "            open_pr = pr\n",
    "\n",
    "# Get corpus document for open PR\n",
    "open_pr_corpus = str(open_pr.title) + \"\\n\" + str(open_pr.body)\n",
    "for comment in open_pr.get_issue_comments():\n",
    "    open_pr_corpus += comment.body\n",
    "\n",
    "# Remove the code, mentions and URLS\n",
    "open_pr_corpus = re.sub(r\"\\```[^```]*\\```\", \"\", open_pr_corpus)\n",
    "open_pr_corpus = re.sub(r\"(?:\\@|#|https?\\://)\\S+\", \"\", open_pr_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7\n",
    "\n",
    "Show the current reviewers in the open PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the open PR submitter\n",
    "open_pr_requester = open_pr.user.login\n",
    "\n",
    "# Get the actual reviewers of open PR\n",
    "open_pr_reviewers = []\n",
    "for comment in open_pr.get_issue_comments():\n",
    "    reviewer = comment.user.login\n",
    "    # Exclude bot, maintainer and PR submitter\n",
    "    if open_pr_requester != reviewer and reviewer not in open_pr_reviewers and 'bot' not in reviewer and repo_maintainer != reviewer:\n",
    "        open_pr_reviewers.append(reviewer)\n",
    "\n",
    "# Remove the open PR reviewers that are not in our graph\n",
    "for open_pr_rv in open_pr_reviewers:\n",
    "    if open_pr_rv not in closed_prs_reviewers:\n",
    "        open_pr_reviewers.remove(open_pr_rv)\n",
    "        \n",
    "print(open_pr_reviewers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "\n",
    "Define the cosine similarity functions with helper function for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import nltk\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import simple_preprocess\n",
    "from github import Github\n",
    "from networkx.algorithms import bipartite\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "np.random.seed(2018)\n",
    "nltk.download('wordnet')\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(stemmer.stem(\n",
    "                WordNetLemmatizer().lemmatize(token, pos='v')))\n",
    "    return result\n",
    "\n",
    "\n",
    "def lda_cosine_sim(closed_prs_meta, closed_prs_corpus, open_pr_corpus):\n",
    "    \n",
    "    corpus_data = []\n",
    "    \n",
    "    # preprocess each of documents and insert in corpus data\n",
    "    for i, pr in enumerate(closed_prs_meta):\n",
    "        preprocessed_data = preprocess(closed_prs_corpus[pr['id']])\n",
    "        corpus_data.append(preprocessed_data)\n",
    "    # Also, add preprocessed open PR document to end of corpus data\n",
    "    corpus_data.append(preprocess(open_pr_corpus))\n",
    "    \n",
    "    # Map between normalized words and integer ID in dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(corpus_data)\n",
    "    \n",
    "    # FIlter the dictionary items\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.8, keep_n=100000)\n",
    "    \n",
    "    # Convert the documents into bag of words format\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus_data]\n",
    "    \n",
    "    # Apply TF-IDF in bag of words\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "    \n",
    "    # Apply LDA to TF_IDF corpus\n",
    "    lda_model_tfidf = gensim.models.LdaMulticore(\n",
    "        corpus_tfidf, num_topics=100, id2word=dictionary, passes=2, workers=4)\n",
    "    \n",
    "    # Compare the open PR and closed PRs documents corpus to get cosine similarity\n",
    "    similarity_matrix = []\n",
    "    for i in range(len(corpus_data) - 1):\n",
    "        sim = gensim.matutils.cossim(\n",
    "            lda_model_tfidf[bow_corpus][i], lda_model_tfidf[bow_corpus][len(corpus_data) - 1])\n",
    "        similarity_matrix.append(sim)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9\n",
    "\n",
    "Get similarity matrix result from lda cosine similarity function passing the corpus document of all closed PRs and one open PR\n",
    "\n",
    "Get the similarity matrix result as defined by similarity threshold\n",
    "\n",
    "Print top 5 similarity matrix result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import gensim\n",
    "\n",
    "# Set similarity threshold\n",
    "similarity_threshold = 0.2\n",
    "\n",
    "# Get the similarity matrix between all the closed PRs and open PR\n",
    "similarity_matrix = lda_cosine_sim(closed_prs_meta, closed_prs_corpus, open_pr_corpus)\n",
    "\n",
    "# Sort the similarity matrix in reverse order\n",
    "similarity_matrix = sorted(similarity_matrix, reverse=True)\n",
    "\n",
    "# Get the top similarity matrix greater than similarity threshold\n",
    "top_similarity_matrix = {}\n",
    "for i, pr in enumerate(closed_prs_meta):\n",
    "    top_similarity_matrix[pr['id']] = similarity_matrix[i]\n",
    "    \n",
    "# Get top similarity matrix using similarity threshold value\n",
    "top_similarity_matrix = dict(itertools.islice(top_similarity_matrix.items(), int(len(top_similarity_matrix)*similarity_threshold)))\n",
    "\n",
    "# print number of PR choosen\n",
    "print(\"Number of closed PR selected: \", len(top_similarity_matrix))\n",
    "\n",
    "# Print top 5 PRs with similarity score\n",
    "print(\"####Similarity score of pull requests ####\")\n",
    "show_5_similarity_matrix = dict(itertools.islice(top_similarity_matrix.items(),5))\n",
    "for key in show_5_similarity_matrix:\n",
    "        print(\"{0} {1} {2}\".format(key, 'PR #' + str(open_pr.number), top_similarity_matrix[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10\n",
    "\n",
    "Define custom weight for bipartite graph projection. It uses weights from bipartite graph and similarity score to calculate new weights.\n",
    "\n",
    "Remove unwanted PRs nodes from bipartite graph to form a bipartite subgraph.\n",
    "\n",
    "Project the bipartite subgraph to build a reviewer's only graph.\n",
    "\n",
    "Then remove isolated reviewer nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_weight(G, u, v, weight='weight'):\n",
    "    weight_val = 0\n",
    "    for nbr in set(G[u]) & set(G[v]):\n",
    "        weight_val += (G[u][nbr]['weight'] + G[v][nbr]['weight'])*G.nodes[nbr]['similarity']\n",
    "    return weight_val\n",
    "\n",
    "\n",
    "# Copy the bipartite graph into new onw\n",
    "copied_barpartite_graphz = graphz.copy()\n",
    "\n",
    "# Get the top PR from similarity rank\n",
    "pr_nodes = []\n",
    "for similarity_id in top_similarity_matrix:\n",
    "    pr_nodes.append(similarity_id)\n",
    "\n",
    "# Remove PR nodes other than top selected PR nodes\n",
    "for node in list(copied_barpartite_graphz.nodes):\n",
    "    if 'PR #' in node and node not in pr_nodes and copied_barpartite_graphz.has_node(node):\n",
    "        copied_barpartite_graphz.remove_node(node)\n",
    "\n",
    "# Insert similarity scores in PR nodes for further use in custom weight\n",
    "for node in copied_barpartite_graphz.nodes:\n",
    "    if node in pr_nodes:\n",
    "        copied_barpartite_graphz.nodes[node]['similarity'] = top_similarity_matrix[node]\n",
    "\n",
    "# Initialize a projected graph\n",
    "projected_graphz = nx.Graph()\n",
    "\n",
    "# Project the copied bipartate graph into reviewers graph considering the weights\n",
    "projected_graphz = bipartite.generic_weighted_projected_graph(\n",
    "    copied_barpartite_graphz, closed_prs_reviewers, weight_function=custom_weight)\n",
    "\n",
    "# Remove isolatated nodes from the projected graph\n",
    "for node in list(nx.isolates(projected_graphz)):\n",
    "    projected_graphz.remove_node(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11\n",
    "\n",
    "Visualize the projected reviewer's only graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.kamada_kawai_layout(graphz, weight='weight')\n",
    "plt.figure(figsize=(10, 7), dpi=300)\n",
    "plt.rcParams['legend.loc'] = 'upper right'\n",
    "\n",
    "plt.plot([0],[0], color=\"green\", label='Reviewers')\n",
    "plt.legend()\n",
    "\n",
    "nx.draw_networkx(projected_graphz, pos=pos, node_size=100, node_color=\"red\",\n",
    "                 with_labels=False, width=0.5, edge_color=\"#ffcccb\")\n",
    "\n",
    "figname = 'projected_graph'\n",
    "plt.title(\"Projected reviewer's graph\")\n",
    "plt.savefig(figname+'.png', bbox_inches='tight', dpi=500)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# save gephi graph file for visualization\n",
    "# nx.write_gexf(projected_graphz, \"projected_graph.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12\n",
    "\n",
    "Show the information of reviewer's graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(projected_graphz))\n",
    "print(\"Average clustering:\", nx.average_clustering(projected_graphz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13\n",
    "\n",
    "Apply page rank algorithm and display the results\n",
    "\n",
    "The reviewer results are the recommended reviewers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of recommendation results to show\n",
    "limit_recomm = 5\n",
    "\n",
    "# Run page rank algorithm in projected graph\n",
    "pagerank = nx.pagerank(projected_graphz, alpha=0.85, personalization=None, max_iter=100, tol=1e-06,\n",
    "                       nstart=None, weight='weight', dangling=None)\n",
    "# Sort the page rank result by score\n",
    "pagerank = list(sorted(pagerank.items(), reverse=True, key=lambda x: x[1]))\n",
    "\n",
    "# If there is recommendation limitation, limit it\n",
    "if limit_recomm != None:\n",
    "    pagerank = pagerank[:limit_recomm]\n",
    "\n",
    "# Get only users from page rank result\n",
    "pagerank_reviewers = [pg[0] for pg in pagerank]\n",
    "\n",
    "# Print page rank scores\n",
    "print(\"Page rank scores\")\n",
    "print(pagerank)\n",
    "\n",
    "# Print the current reviewers\n",
    "print(\"Current reviewers\")\n",
    "print(open_pr_reviewers)\n",
    "\n",
    "# Print the recommended reviewers\n",
    "print(\"Recommended Reviewers\")\n",
    "print(pagerank_reviewers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14\n",
    "\n",
    "Show the accuracy results\n",
    "Also, show confusion matrix and other metrics like precision, recall, and f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "page_rank_result = []\n",
    "\n",
    "# Get the predicted reviewers\n",
    "for opr_reviewer in open_pr_reviewers:\n",
    "    if opr_reviewer in pagerank_reviewers:\n",
    "        page_rank_result.append(opr_reviewer)\n",
    "    else:\n",
    "        page_rank_result.append(\"\")\n",
    "\n",
    "# Define true and predicted reviewers to getr accuracy\n",
    "y_true = np.array(open_pr_reviewers)\n",
    "y_pred = np.array(page_rank_result)\n",
    "\n",
    "#  Get the accuracy score\n",
    "x_accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Print the accuracy result\n",
    "print(\"Accuracy:\", x_accuracy)\n",
    "\n",
    "x_confusion_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "print(x_confusion_matrix)\n",
    "\n",
    "# Get full report different metrices\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15\n",
    "\n",
    "Generate a accuracy result for random 3 open PRs of the given repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust similarity threshold\n",
    "similarity_threshold = 0.2\n",
    "\n",
    "# Get 3 open PR Ids\n",
    "three_open_prs = [4309, 4559, 4296]\n",
    "\n",
    "# Store the filtered open PRs here\n",
    "open_pull_requests = []\n",
    "\n",
    "# Iterate through all the open pull requests\n",
    "for opq in open_prs:\n",
    "    \n",
    "    # Continue if not in the list\n",
    "    if opq.number not in three_open_prs: continue \n",
    "    \n",
    "    # If open PR has comments less than 2 comments, discard it\n",
    "    if opq.get_issue_comments().totalCount < 2:\n",
    "        print(opq.number)\n",
    "        raise Exception(\"Use open PR with comments greater than 2\")\n",
    "    \n",
    "    # Build a document corpus for the PR\n",
    "    open_pr_doc = str(opq.title) + \"\\n\" + str(opq.body)\n",
    "\n",
    "    opq_requester = opq.user.login\n",
    "    open_pr_reviewers = []\n",
    "    for comment in opq.get_issue_comments():\n",
    "        open_pr_doc += comment.body\n",
    "        reviewer = comment.user.login\n",
    "        if opq_requester != reviewer and reviewer not in open_pr_reviewers and 'bot' not in reviewer:\n",
    "            open_pr_reviewers.append(reviewer)\n",
    "    \n",
    "    # Remove the code, mentions and URLS\n",
    "    open_pr_doc = re.sub(r\"\\```[^```]*\\```\", \"\", open_pr_doc)\n",
    "    open_pr_doc = re.sub(r\"(?:\\@|#|https?\\://)\\S+\", \"\", open_pr_doc)\n",
    "    \n",
    "    # Get the reviewers of the open PR\n",
    "    for new_rev in list(open_pr_reviewers):\n",
    "        if new_rev not in closed_prs_reviewers:\n",
    "            open_pr_reviewers.remove(new_rev)\n",
    "    \n",
    "    # Append the filtered PR data\n",
    "    open_pull_requests.append((opq.number, open_pr_doc, open_pr_reviewers))\n",
    "\n",
    "# Iterate through filtered open PRs and measure accuracy\n",
    "pr_metrices = []\n",
    "for opqid, open_pr_doc, open_pr_reviewers in open_pull_requests:\n",
    "    \n",
    "    # Get the similarity matrix between all the closed PRs and open PR\n",
    "    similarity_matrix = lda_cosine_sim(closed_prs_meta, closed_prs_corpus, open_pr_doc)\n",
    "    \n",
    "    # Sort the similarity matrix in reverse order\n",
    "    similarity_matrix = sorted(similarity_matrix, reverse=True)\n",
    "    \n",
    "    # Get the top similarity matrix by PR and filter with threshold\n",
    "    top_similarity_matrix = {}\n",
    "    for i, pr in enumerate(closed_prs_meta):\n",
    "        top_similarity_matrix[pr['id']] = similarity_matrix[i]\n",
    "\n",
    "    # Get top similarity matrix using similarity threshold value\n",
    "    top_similarity_matrix = dict(itertools.islice(top_similarity_matrix.items(), int(len(top_similarity_matrix)*similarity_threshold)))\n",
    "    \n",
    "    # Copy the bipartite graph into new one\n",
    "    copied_barpartite_graphz = graphz.copy()\n",
    "\n",
    "    # Get the top PR from similarity rank\n",
    "    pr_nodes = []\n",
    "    for similarity_id in top_similarity_matrix:\n",
    "        pr_nodes.append(similarity_id)\n",
    "    \n",
    "    # Remove PR nodes other than top selected PR nodes\n",
    "    for node in list(copied_barpartite_graphz.nodes):\n",
    "        if 'PR #' in node and node not in pr_nodes and copied_barpartite_graphz.has_node(node):\n",
    "            copied_barpartite_graphz.remove_node(node)\n",
    "            \n",
    "    # Insert similarity scores in PR nodes for further use in custom weight\n",
    "    for node in copied_barpartite_graphz.nodes:\n",
    "        if node in pr_nodes:\n",
    "            copied_barpartite_graphz.nodes[node]['similarity'] = top_similarity_matrix[node]\n",
    "\n",
    "    # Initialize a projected graph\n",
    "    projected_graphz = nx.Graph()\n",
    "\n",
    "    # Project the copied bipartate graph into reviewers graph considering the weights\n",
    "    projected_graphz = bipartite.generic_weighted_projected_graph(\n",
    "        copied_barpartite_graphz, closed_prs_reviewers, weight_function=custom_weight)\n",
    "\n",
    "    # Remove isolatated nodes from the projected graph\n",
    "    for node in list(nx.isolates(projected_graphz)):\n",
    "        projected_graphz.remove_node(node)\n",
    "\n",
    "    # Run page rank algorithm in projected graph\n",
    "    pagerank = nx.pagerank(projected_graphz, alpha=0.85, personalization=None, max_iter=100, tol=1e-06,\n",
    "                           nstart=None, weight='weight', dangling=None)\n",
    "    \n",
    "    # get the accuracy from page rank results\n",
    "    page_rank_metrics = []\n",
    "    \n",
    "    # Iterate through number of recommendations\n",
    "    for i in range(1, 6):\n",
    "        \n",
    "        # sort the page rank result\n",
    "        pagerank_sorted = sorted(pagerank.items(), reverse=True, key=lambda x: x[1])\n",
    "        \n",
    "        # Get only reviewers name from pagerank result\n",
    "        pagerank_reviewers = [pg[0] for pg in pagerank_sorted][:i]\n",
    "        \n",
    "        # Get the predicted reviewers\n",
    "        page_rank_result = []\n",
    "        for opr_reviewer in open_pr_reviewers:\n",
    "            if opr_reviewer in pagerank_reviewers:\n",
    "                page_rank_result.append(opr_reviewer)\n",
    "            else:\n",
    "                page_rank_result.append(\"\")\n",
    "        \n",
    "        # Define true and predicted values for finding accuracy\n",
    "        pg_y_true = np.array(open_pr_reviewers)\n",
    "        pg_y_pred = np.array(page_rank_result)\n",
    "        \n",
    "        # Get the accuracy score\n",
    "        pg_accuracy = accuracy_score(pg_y_true, pg_y_pred)\n",
    "        \n",
    "        # Append the accuracy socre to main list\n",
    "        page_rank_metrics.append(pg_accuracy)\n",
    "    \n",
    "    # Append into whole result\n",
    "    pr_metrices.append(page_rank_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16\n",
    "\n",
    "Plot the accuracy result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_pr_metrices = pr_metrices[:3]\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, 'axes.titlesize': 12})\n",
    "fig, axs = plt.subplots(1, len(truncated_pr_metrices), figsize=(20, 4), dpi=300)\n",
    "\n",
    "for i in range(0, len(truncated_pr_metrices)):\n",
    "    \n",
    "    # Plot the accuracy metrices\n",
    "    axs[i].plot(range(1, 6), pr_metrices[i], label=\"Page Rank Accuracy Rate\", color='green', marker='o',\n",
    "             linewidth=3, markersize=2)\n",
    "    \n",
    "    # Set plot attributes\n",
    "    axs[i].set_ylabel(\"Accuracy Percentage\")\n",
    "    axs[i].set_xlabel(\"Number of Recommentation reviewers\")\n",
    "    axs[i].set_title(\"Accuracy Graph for PR #{}\".format(open_pull_requests[i][0]))\n",
    "    axs[i].set_ylim(0, 1)\n",
    "    axs[i].set_xlim(1, 5)\n",
    "    axs[i].set_yticks(np.arange(0, 1.2, step=0.2))\n",
    "    axs[i].set_xticks(np.arange(6))\n",
    "    \n",
    "    # show legend\n",
    "    axs[i].legend()\n",
    "\n",
    "figname = 'recomm_plot'\n",
    "plt.savefig(figname+'.png', bbox_inches='tight', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17\n",
    "\n",
    "Get the average accuracy of recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pr = []\n",
    "for x in range(0, 5):\n",
    "    sum_pg = 0\n",
    "    for y in range(0, 3):\n",
    "        sum_pg += pr_metrices[y][x]\n",
    "    sum_pg = sum_pg / 3\n",
    "    avg_pr.append(sum_pg)\n",
    "print(avg_pr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
