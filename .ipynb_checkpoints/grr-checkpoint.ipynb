{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import simple_preprocess\n",
    "from github import Github\n",
    "from networkx.algorithms import bipartite\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "np.random.seed(2018)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "ACCESS_TOKEN = '5feeb874084cea318f3c5cbf56b70bce1a578290'\n",
    "\n",
    "###############################################################################\n",
    "########### Data Preprocessing Helper Function  ###############################\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(stemmer.stem(\n",
    "                WordNetLemmatizer().lemmatize(token, pos='v')))\n",
    "    return result\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "########### LDA FUNCTION WITH COSINE SIMILARITY  ##############################\n",
    "###############################################################################\n",
    "'''\n",
    "This function takes three parameters:\n",
    "closed_prs_metaL List of documents generated from closed pull requests\n",
    "closed_prs_corpus: List of document corpus generated from closed pull requests\n",
    "open_pr_corpus: Document generated for open PR\n",
    "'''\n",
    "\n",
    "\n",
    "def lda_cosine_sim(closed_prs_meta, closed_prs_corpus, open_pr_corpus):\n",
    "\n",
    "    corpus_data = []\n",
    "\n",
    "    # preprocess each of documents and insert in corpus data\n",
    "    for i, pr in enumerate(closed_prs_meta):\n",
    "        preprocessed_data = preprocess(closed_prs_corpus[pr['id']])\n",
    "        corpus_data.append(preprocessed_data)\n",
    "    # Also, add preprocessed open PR document to end of corpus data\n",
    "    corpus_data.append(preprocess(open_pr_corpus))\n",
    "\n",
    "    # Map between normalized words and integer ID in dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(corpus_data)\n",
    "\n",
    "    # FIlter the dictionary items\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.8, keep_n=100000)\n",
    "\n",
    "    # Convert the documents into bag of words format\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus_data]\n",
    "\n",
    "    # Apply TF-IDF in bag of words\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "    # Apply LDA to TF_IDF corpus\n",
    "    lda_model_tfidf = gensim.models.LdaMulticore(\n",
    "        corpus_tfidf, num_topics=100, id2word=dictionary, passes=2, workers=4)\n",
    "\n",
    "    # Compare the open PR and closed PRs documents corpus to get cosine similarity\n",
    "    similarity_matrix = []\n",
    "    for i in range(len(corpus_data) - 1):\n",
    "        sim = gensim.matutils.cossim(\n",
    "            lda_model_tfidf[bow_corpus][i], lda_model_tfidf[bow_corpus][len(corpus_data) - 1])\n",
    "        similarity_matrix.append(sim)\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "########### CUSTOM WEIGHT FUNCTION FOR PROJECTION #############################\n",
    "###############################################################################\n",
    "'''\n",
    "This function calculate the weight from bipartite graph and transfer into\n",
    "weights of projected graph\n",
    "'''\n",
    "\n",
    "\n",
    "def custom_weight(G, u, v, weight='weight'):\n",
    "    weight_val = 0\n",
    "    for nbr in set(G[u]) & set(G[v]):\n",
    "        # Add current weights and multiply by PR similarity score\n",
    "        weight_val += (G[u][nbr]['weight'] + G[v][nbr]\n",
    "                       ['weight'])*G.nodes[nbr]['similarity']\n",
    "    return weight_val\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "########### MAIN REVIEWER RECOMMENDATION FUNCTION #############################\n",
    "###############################################################################\n",
    "'''\n",
    "This function takes the following parameters:\n",
    "repo_name: Name of repository in github\n",
    "access_token: Access token to use Github API\n",
    "limit_pr: Value to limit number of closed PRs to process\n",
    "open_pr_id: Open pull request ID. Typically found in github\n",
    "pull requests section of repository\n",
    "limit_recomm: Limit the number of recommendation\n",
    "similarity_threshold: consine similarity tuning parameter\n",
    "'''\n",
    "\n",
    "\n",
    "def get_reviewer_recommendation(repo_name, access_token, open_pr_id=None, similarity_threshold=0.2, limit_pr=None, limit_recomm=5):\n",
    "\n",
    "    # Get the access to Github API\n",
    "    client = Github(access_token, per_page=300)\n",
    "\n",
    "    # Get the repository object from Github API\n",
    "    repo = client.get_repo(repo_name)\n",
    "\n",
    "    # Get the maintainer of the repo\n",
    "    repo_maintainer = repo.full_name.split(\"/\")[0]\n",
    "\n",
    "    # Get the list of closed PRS\n",
    "    open_prs = list(repo.get_pulls(state='open', sort='created'))\n",
    "    if len(open_prs) == 0:\n",
    "        raise Exception(\n",
    "            \"Insufficient number of open pull requests. Use different repository.\")\n",
    "\n",
    "    # Get the first open PR\n",
    "    open_pr = open_prs[0]\n",
    "\n",
    "    # If Id is provided in function, choose this one\n",
    "    if open_pr_id != None:\n",
    "        for pr in open_prs:\n",
    "            if open_pr_id == pr.number:\n",
    "                open_pr = pr\n",
    "\n",
    "    if open_pr_id != None and open_pr_id != open_pr.number:\n",
    "        raise Exception(\"Open PR not found. Change Open PR ID.\")\n",
    "\n",
    "    # Get all the closed pull requests\n",
    "    closed_prs = list(repo.get_pulls(state='closed'))\n",
    "\n",
    "    if len(closed_prs) < 1:\n",
    "        raise Exception(\n",
    "            \"Insufficient number of closed pull requests. Use different repository.\")\n",
    "\n",
    "    # Limit number of pull requests if limit_pr is set\n",
    "    if limit_pr != None and limit_pr < len(closed_prs):\n",
    "        closed_prs = closed_prs[:limit_pr]\n",
    "\n",
    "    # Initialize a graph\n",
    "    graphz = nx.Graph()\n",
    "\n",
    "    # It inserts all the reviewers node we add to graph\n",
    "    closed_prs_reviewers = []\n",
    "\n",
    "    # Save the data loaded from API for future use\n",
    "    closed_prs_meta = []\n",
    "\n",
    "    # Iterate through all the closed pull requests\n",
    "    for pr in closed_prs:\n",
    "\n",
    "        # If PR doesnt have comments continue with next\n",
    "        if pr.get_issue_comments().totalCount == 0:\n",
    "            continue\n",
    "\n",
    "        # Get the user who submitted this PR\n",
    "        pull_requester = pr.user.login\n",
    "\n",
    "        # Get the PR number\n",
    "        pr_number = 'PR #' + str(pr.number)\n",
    "\n",
    "        # Insert PR into graph node\n",
    "        graphz.add_node(pr_number, type='Pull Request', bipartite=0)\n",
    "\n",
    "        # Get all the comments of the PR\n",
    "        comments = pr.get_issue_comments()\n",
    "\n",
    "        # Get the meta data from PR and insert in closed_prs_meta\n",
    "        pr_data = {}\n",
    "        pr_data['id'] = pr_number\n",
    "        pr_data['title'] = pr.title\n",
    "        pr_data['body'] = pr.body\n",
    "        pr_data['comments'] = comments\n",
    "        closed_prs_meta.append(pr_data)\n",
    "\n",
    "        # Iterate through all the comments\n",
    "        for comment in comments:\n",
    "\n",
    "            # Exclude user who are bots, maintainer, or PR submitter\n",
    "            if comment.user != None and 'bot' not in comment.user.login and repo_maintainer != comment.user.login and pull_requester != comment.user.login:\n",
    "\n",
    "                # Get the reviewer from comment\n",
    "                reviewer = comment.user.login\n",
    "\n",
    "                # Insert reviewer into graph node and closed_prs_reviewers list\n",
    "                if reviewer not in closed_prs_reviewers:\n",
    "                    closed_prs_reviewers.append(reviewer)\n",
    "                    graphz.add_node(reviewer, type='user', bipartite=1)\n",
    "\n",
    "                # If there is occurence of multiple comment, then add the occurence to the edge weight\n",
    "                if graphz.has_edge(reviewer, pr_number):\n",
    "                    # Increment weight of edge\n",
    "                    new_weight = graphz.get_edge_data(\n",
    "                        reviewer, pr_number)['weight'] + 1\n",
    "                    graphz[reviewer][pr_number]['weight'] = new_weight\n",
    "                else:\n",
    "                    # Add edge with weight 1\n",
    "                    graphz.add_edge(reviewer, pr_number,\n",
    "                                    weight=1, type='reviews')\n",
    "\n",
    "    # Generate document corpus for closed pull requests\n",
    "    closed_prs_corpus = {}\n",
    "    for pr in closed_prs_meta:\n",
    "        title = str(pr['title'])\n",
    "        body = str(pr['body'])\n",
    "        doc = title + \" \" + body\n",
    "        for comment in pr['comments']:\n",
    "            doc += comment.body\n",
    "        # Remove the code, mentions and URLS\n",
    "        doc = re.sub(r'\\```[^```]*\\```', '', doc)\n",
    "        doc = re.sub(r\"(?:\\@|#|https?\\://)\\S+\", \"\", doc)\n",
    "\n",
    "        # insert document into corpus with index of corpus id\n",
    "        closed_prs_corpus[pr['id']] = doc\n",
    "\n",
    "    # Get corpus document for open PR\n",
    "    open_pr_corpus = str(open_pr.title) + \"\\n\" + str(open_pr.body)\n",
    "    for comment in open_pr.get_issue_comments():\n",
    "        open_pr_corpus += comment.body\n",
    "\n",
    "    # Remove the code, mentions and URLS\n",
    "    open_pr_corpus = re.sub('`.*`', '', open_pr_corpus)\n",
    "    open_pr_corpus = re.sub(r\"(?:\\@|#|https?\\://)\\S+\", \"\", open_pr_corpus)\n",
    "\n",
    "    # Get the open PR submitter\n",
    "    open_pr_requester = open_pr.user.login\n",
    "\n",
    "    # Get the actual reviewers of open PR\n",
    "    open_pr_reviewers = []\n",
    "    for comment in open_pr.get_issue_comments():\n",
    "        reviewer = comment.user.login\n",
    "        # Exclude bot, maintainer and PR submitter\n",
    "        if open_pr_requester != reviewer and reviewer not in open_pr_reviewers and 'bot' not in reviewer and repo_maintainer != reviewer:\n",
    "            open_pr_reviewers.append(reviewer)\n",
    "\n",
    "    # Remove the open PR reviewers that are not in our graph\n",
    "    for open_pr_rv in open_pr_reviewers:\n",
    "        if open_pr_rv not in closed_prs_reviewers:\n",
    "            open_pr_reviewers.remove(open_pr_rv)\n",
    "\n",
    "    # Get the similarity matrix between all the closed PRs and open PR\n",
    "    similarity_matrix = lda_cosine_sim(\n",
    "        closed_prs_meta, closed_prs_corpus, open_pr_corpus)\n",
    "\n",
    "    # Sort the similarity matrix in reverse order\n",
    "    similarity_matrix = sorted(similarity_matrix, reverse=True)\n",
    "\n",
    "    # Get all similarity matrix filtered with threshold\n",
    "    top_similarity_matrix = {}\n",
    "    for i, pr in enumerate(closed_prs_meta):\n",
    "        top_similarity_matrix[pr['id']] = similarity_matrix[i]\n",
    "\n",
    "    # Get top similarity matrix using similarity threshold value\n",
    "    top_sim_length = int(len(top_similarity_matrix)*similarity_threshold)\n",
    "    top_similarity_matrix = dict(itertools.islice(\n",
    "        top_similarity_matrix.items(), top_sim_length))\n",
    "\n",
    "    # Copy the bipartite graph into new one\n",
    "    copied_barpartite_graphz = graphz.copy()\n",
    "\n",
    "    # Get the top PR from similarity rank\n",
    "    pr_nodes = []\n",
    "    for similarity_id in top_similarity_matrix:\n",
    "        pr_nodes.append(similarity_id)\n",
    "\n",
    "    # Remove PR nodes other than top selected PR nodes\n",
    "    for node in list(copied_barpartite_graphz.nodes):\n",
    "        if 'PR #' in node and node not in pr_nodes and copied_barpartite_graphz.has_node(node):\n",
    "            copied_barpartite_graphz.remove_node(node)\n",
    "\n",
    "    # Insert similarity scores in PR nodes for further use in custom weight\n",
    "    for node in copied_barpartite_graphz.nodes:\n",
    "        if node in pr_nodes:\n",
    "            copied_barpartite_graphz.nodes[node]['similarity'] = top_similarity_matrix[node]\n",
    "\n",
    "    # Initialize a projected graph\n",
    "    projected_graphz = nx.Graph()\n",
    "\n",
    "    # Project the copied bipartate graph into reviewers graph considering the weights\n",
    "    projected_graphz = bipartite.generic_weighted_projected_graph(\n",
    "        copied_barpartite_graphz, closed_prs_reviewers, weight_function=custom_weight)\n",
    "\n",
    "    # Remove isolatated nodes from the projected graph\n",
    "    for node in list(nx.isolates(projected_graphz)):\n",
    "        projected_graphz.remove_node(node)\n",
    "\n",
    "    if len(projected_graphz.nodes) == 0:\n",
    "        raise Exception(\"Use more similarity threshold.\")\n",
    "\n",
    "    # Run page rank algorithm in projected graph\n",
    "    pagerank = nx.pagerank(projected_graphz, alpha=0.85, personalization=None,\n",
    "                           max_iter=100, tol=1e-06, nstart=None, weight='weight', dangling=None)\n",
    "\n",
    "    # Sort the page rank result by score\n",
    "    pagerank = list(sorted(pagerank.items(), reverse=True, key=lambda x: x[1]))\n",
    "\n",
    "    # Get only users from page rank result\n",
    "    pagerank_reviewers = [pg[0] for pg in pagerank]\n",
    "\n",
    "    # If there is recommendation limitation, limit it\n",
    "    if limit_recomm != None:\n",
    "        pagerank_reviewers = pagerank_reviewers[:limit_recomm]\n",
    "\n",
    "    # Print the current reviewers\n",
    "    print(\"Current reviewers\", open_pr_reviewers)\n",
    "\n",
    "    # Print the recommended reviewers\n",
    "    print(\"Recommended reviewers\", pagerank_reviewers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Open PR not found. Change Open PR ID.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d7db5a2bf509>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_reviewer_recommendation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREPO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mACCESS_TOKEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit_pr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen_pr_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOPEN_PR_ID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarity_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-0e39affee4aa>\u001b[0m in \u001b[0;36mget_reviewer_recommendation\u001b[1;34m(repo_name, access_token, open_pr_id, similarity_threshold, limit_pr, limit_recomm)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mopen_pr_id\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mopen_pr\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mopen_pr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Open PR not found. Change Open PR ID.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;31m# Get all the closed pull requests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Open PR not found. Change Open PR ID."
     ]
    }
   ],
   "source": [
    "REPO = 'mrdoob/three.js'\n",
    "OPEN_PR_ID = 19266\n",
    "\n",
    "get_reviewer_recommendation(REPO, ACCESS_TOKEN, limit_pr=500, open_pr_id=OPEN_PR_ID, similarity_threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reviewer_recommendation(REPO, ACCESS_TOKEN, limit_pr=500, open_pr_id=19144, similarity_threshold=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
